{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvSXQthsbNG6"
      },
      "source": [
        "## Pre trained embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qj6SugubNG6"
      },
      "source": [
        "Word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of  techniques, some involving neural networks, others not. The idea of a dense, lowdimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s,1 but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the Word2vec algorithm (https://code.google.com/ archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2vec dimensions capture specific semantic properties, such as gender.\n",
        "\n",
        "There are various precomputed databases of word embeddings that you can download and use in a Keras Embedding layer. Word2vec is one of them. Another popular one is called Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data.\n",
        "\n",
        "One of the most widely used pretrained word embeddings is Glove and can be downloaded from https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "GloVe is pre-computed embeddings from 2014 English Wikipedia. It's a 822MB zip file named glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "FA3EGyDf7wHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d7e175-983d-4c84-c3f7-80a341f97fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-14 06:32:02--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-10-14 06:32:02--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.96MB/s    in 2m 45s  \n",
            "\n",
            "2023-10-14 06:34:47 (4.98 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir glove\n",
        "!unzip glove.6B.zip -d glove/"
      ],
      "metadata": {
        "id": "uJN8PVIW75jw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f85b586-f77c-4682-91cf-bd5d4cdf6405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove/glove.6B.50d.txt  \n",
            "  inflating: glove/glove.6B.100d.txt  \n",
            "  inflating: glove/glove.6B.200d.txt  \n",
            "  inflating: glove/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 /content/glove/glove.6B.50d.txt"
      ],
      "metadata": {
        "id": "nJuA1AaVAyl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0390cf-c119-42a1-ac23-061868c6ad86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
            ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
            ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
            "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
            "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n",
            "and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n",
            "in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n",
            "a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n",
            "\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n",
            "'s 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n",
            "for 0.15272 0.36181 -0.22168 0.066051 0.13029 0.37075 -0.75874 -0.44722 0.22563 0.10208 0.054225 0.13494 -0.43052 -0.2134 0.56139 -0.21445 0.077974 0.10137 -0.51306 -0.40295 0.40639 0.23309 0.20696 -0.12668 -0.50634 -1.7131 0.077183 -0.39138 -0.10594 -0.23743 3.9552 0.66596 -0.61841 -0.3268 0.37021 0.25764 0.38977 0.27121 0.043024 -0.34322 0.020339 0.2142 0.044097 0.14003 -0.20079 0.074794 -0.36076 0.43382 -0.084617 0.1214\n",
            "- -0.16768 1.2151 0.49515 0.26836 -0.4585 -0.23311 -0.52822 -1.3557 0.16098 0.37691 -0.92702 -0.43904 -1.0634 1.028 0.0053943 0.04153 -0.018638 -0.55451 0.026166 0.28066 -0.66245 0.23435 0.2451 0.025668 -1.0869 -2.844 -0.51272 0.27286 0.0071502 0.033984 3.9084 0.52766 -0.66899 1.8238 0.43436 -0.30084 -0.26996 0.4394 0.69956 0.14885 0.029453 1.4888 0.52361 0.099354 1.2515 0.099381 -0.079261 -0.30862 0.30893 0.11023\n",
            "that 0.88387 -0.14199 0.13566 0.098682 0.51218 0.49138 -0.47155 -0.30742 0.01963 0.12686 0.073524 0.35836 -0.60874 -0.18676 0.78935 0.54534 0.1106 -0.2923 0.059041 -0.69551 -0.18804 0.19455 0.32269 -0.49981 0.306 -2.3902 -0.60749 0.37107 0.078912 -0.23896 3.839 -0.20355 -0.35613 -0.69185 -0.17497 -0.35323 0.10598 -0.039303 0.015701 0.038279 -0.35283 0.44882 -0.16534 0.31579 0.14963 -0.071277 -0.53506 0.52711 -0.20148 0.0095952\n",
            "on 0.30045 0.25006 -0.16692 0.1923 0.026921 -0.079486 -0.91383 -0.1974 -0.053413 -0.40846 -0.26844 -0.28212 -0.5 0.1221 0.3903 0.17797 -0.4429 -0.40478 -0.9505 -0.16897 0.77793 0.33525 0.3346 -0.1754 -0.12017 -1.7861 0.29241 0.55933 0.029982 -0.32417 3.9297 0.1088 -0.57335 -0.17842 0.0041748 -0.16309 0.45077 -0.16123 -0.17311 -0.087889 -0.089032 0.062001 -0.19946 -0.38863 -0.18232 0.060751 0.098603 -0.07131 0.23052 -0.51939\n",
            "is 0.6185 0.64254 -0.46552 0.3757 0.74838 0.53739 0.0022239 -0.60577 0.26408 0.11703 0.43722 0.20092 -0.057859 -0.34589 0.21664 0.58573 0.53919 0.6949 -0.15618 0.05583 -0.60515 -0.28997 -0.025594 0.55593 0.25356 -1.9612 -0.51381 0.69096 0.066246 -0.054224 3.7871 -0.77403 -0.12689 -0.51465 0.066705 -0.32933 0.13483 0.19049 0.13812 -0.21503 -0.016573 0.312 -0.33189 -0.026001 -0.38203 0.19403 -0.12466 -0.27557 0.30899 0.48497\n",
            "was 0.086888 -0.19416 -0.24267 -0.33391 0.56731 0.39783 -0.97809 0.03159 -0.61469 -0.31406 0.56145 0.12886 -0.84193 -0.46992 0.47097 0.023012 -0.59609 0.22291 -1.1614 0.3865 0.067412 0.44883 0.17394 -0.53574 0.17909 -2.1647 -0.12827 0.29036 -0.15061 0.35242 3.124 -0.90085 -0.02567 -0.41709 0.40565 -0.22703 0.76829 0.60982 0.070068 -0.13271 -0.1201 0.096132 -0.43998 -0.48531 -0.5188 -0.3077 -0.75028 -0.77 0.3945 -0.16937\n",
            "said 0.38973 -0.2121 0.51837 0.80136 1.0336 -0.27784 -0.84525 -0.25333 0.12586 -0.90342 0.24975 0.22022 -1.2053 -0.53771 1.0446 0.62778 0.39704 -0.15812 0.38102 -0.54674 -0.44009 1.0976 0.013069 -0.89971 0.41226 -2.2309 0.28997 0.32175 -0.72738 -0.092244 3.028 -0.062599 0.038329 0.0072918 -0.35388 -0.92256 0.097932 0.10068 1.2116 0.88233 -0.46297 1.3186 0.32705 -0.73446 0.89301 -0.45324 -1.2698 0.86119 0.1415 1.2018\n",
            "with 0.25616 0.43694 -0.11889 0.20345 0.41959 0.85863 -0.60344 -0.31835 -0.6718 0.003984 -0.075159 0.11043 -0.73534 0.27436 0.054015 -0.23828 -0.13767 0.011573 -0.46623 -0.55233 0.083317 0.55938 0.51903 -0.27065 -0.28211 -1.3918 0.17498 0.26586 0.061449 -0.273 3.9032 0.38169 -0.056009 -0.004425 0.24033 0.30675 -0.12638 0.33436 0.075485 -0.036218 0.13691 0.37762 -0.12159 -0.13808 0.19505 0.22793 -0.17304 -0.07573 -0.25868 -0.39339\n",
            "he -0.20092 -0.060271 -0.61766 -0.8444 0.5781 0.14671 -0.86098 0.6705 -0.86556 -0.18234 0.15856 0.45814 -1.0163 -0.35874 0.73869 -0.24048 -0.33893 0.25742 -0.78192 0.083528 0.1775 0.91773 0.64531 -0.19896 0.37416 -2.7525 -0.091586 0.040349 -0.064792 -0.31466 3.3944 0.044941 -0.55038 -0.65334 0.10436 0.016394 0.24388 1.0085 0.31412 -0.33806 -0.16925 0.10228 -0.62143 0.19829 -0.36147 -0.24769 -0.38989 -0.33317 -0.041659 -0.013171\n",
            "as 0.20782 0.12713 -0.30188 -0.23125 0.30175 0.33194 -0.52776 -0.44042 -0.48348 0.03502 0.34782 0.54574 -0.2066 -0.083713 0.2462 0.15931 -0.0031349 0.32443 -0.4527 -0.22178 0.022652 -0.041714 0.31815 0.088633 -0.03801 -1.8212 -0.50917 -0.097544 -0.08953 0.050476 3.718 -0.16503 -0.078733 -0.57101 0.20418 0.13411 0.074281 0.087502 -0.25443 -0.15011 -0.15768 0.39606 -0.23646 -0.095054 0.07859 -0.012305 -0.49879 -0.35301 0.05058 0.019495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the embeddings"
      ],
      "metadata": {
        "id": "YqeTLMGTHKs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word2vec_output_file = \"/content/glove/glove.6B.50d.txt\""
      ],
      "metadata": {
        "id": "EQEBccoWG_ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_w2v_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False, no_header=True)"
      ],
      "metadata": {
        "id": "CDSlnOYqHFXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_w2v_model.most_similar('bangalore')"
      ],
      "metadata": {
        "id": "CzodvN1LHbo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7050742-3510-4f07-e851-aaa3b99b1fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('chennai', 0.9154870510101318),\n",
              " ('hyderabad', 0.8739371299743652),\n",
              " ('kolkata', 0.8573629260063171),\n",
              " ('ahmedabad', 0.8511857390403748),\n",
              " ('pune', 0.8379703164100647),\n",
              " ('kanpur', 0.8285415768623352),\n",
              " ('patna', 0.7850483655929565),\n",
              " ('lahore', 0.7841722369194031),\n",
              " ('calcutta', 0.7834596633911133),\n",
              " ('delhi', 0.778948187828064)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_w2v_model.most_similar('dhoni')"
      ],
      "metadata": {
        "id": "c61PbcV0H1sY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab73b62-26a3-492f-cc58-a54f76cc97de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dravid', 0.868076503276825),\n",
              " ('yuvraj', 0.8542092442512512),\n",
              " ('ganguly', 0.8492220640182495),\n",
              " ('kumble', 0.8360502123832703),\n",
              " ('rahul', 0.8356800675392151),\n",
              " ('sehwag', 0.835641622543335),\n",
              " ('laxman', 0.8343917727470398),\n",
              " ('raina', 0.8334249258041382),\n",
              " ('mahendra', 0.8286129236221313),\n",
              " ('karthik', 0.8234096765518188)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_w2v_model.most_similar('india')"
      ],
      "metadata": {
        "id": "0Ri2LbuZH7vW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e664fafa-028a-429a-b887-1507a8d6b62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('indian', 0.8648794293403625),\n",
              " ('pakistan', 0.8529723286628723),\n",
              " ('malaysia', 0.816650927066803),\n",
              " ('bangladesh', 0.8154239058494568),\n",
              " ('delhi', 0.8142766356468201),\n",
              " ('indonesia', 0.7939143776893616),\n",
              " ('thailand', 0.7864410281181335),\n",
              " ('sri', 0.7809486985206604),\n",
              " ('lanka', 0.7792481780052185),\n",
              " ('africa', 0.772837221622467)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_w2v_model.most_similar('lakshmi')"
      ],
      "metadata": {
        "id": "oL_867btJDIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0551c7ea-5434-42ab-f566-0bdb44c245df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('laxmi', 0.7461349964141846),\n",
              " ('parvathi', 0.7299206852912903),\n",
              " ('parvati', 0.7262587547302246),\n",
              " ('shing', 0.7092645764350891),\n",
              " ('devi', 0.705733060836792),\n",
              " ('sethu', 0.6944319605827332),\n",
              " ('ratan', 0.6929410696029663),\n",
              " ('shiva', 0.6819362044334412),\n",
              " ('narasimha', 0.6802400946617126),\n",
              " ('phoolan', 0.664139449596405)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_w2v_model.most_similar('wikipedia')"
      ],
      "metadata": {
        "id": "7CiiUxAnJGaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0583153b-ec71-4c5a-df30-b63da6335566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('german-language', 0.7409663796424866),\n",
              " ('english-language', 0.7226234078407288),\n",
              " ('dictionaries', 0.719719648361206),\n",
              " ('dictionary', 0.7159746885299683),\n",
              " ('publish', 0.7159086465835571),\n",
              " ('website', 0.7043480277061462),\n",
              " ('encyclopedia', 0.700772762298584),\n",
              " ('blog', 0.6988518834114075),\n",
              " ('periodical', 0.6961102485656738),\n",
              " ('editions', 0.6913991570472717)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(a, b, c):\n",
        "    result = pretrained_w2v_model.most_similar([c, b], [a])\n",
        "    return result[0][0]"
      ],
      "metadata": {
        "id": "Re5m3yCpIDvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy('india', 'indian', 'japan')"
      ],
      "metadata": {
        "id": "HN3-w5sMIIiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c7452e10-b0fa-4127-c84b-3536bdb485f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'japanese'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogy('india', 'delhi', 'antarctica')"
      ],
      "metadata": {
        "id": "Vck-uW0WIVKL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3f0d694e-8c05-4585-e6c5-bef797e98185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'glacier'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogy('india', 'dhoni', 'england')"
      ],
      "metadata": {
        "id": "pYVkNww-IdZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1a724f6b-a07b-4522-918c-941c1a741405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'collingwood'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ZUywYW9tbNHA"
      },
      "source": [
        "## Excellent References\n",
        "\n",
        "For further exploration and better understanding, you can use the following references.\n",
        "\n",
        "- Glossary of Deep Learning: Word Embedding\n",
        "\n",
        "    https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca\n",
        "\n",
        "\n",
        "- wevi: word embedding visual inspector\n",
        "\n",
        "    https://ronxin.github.io/wevi/  \n",
        "    \n",
        "    \n",
        "- Learning Word Embedding    \n",
        "\n",
        "    https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html\n",
        "\n",
        "\n",
        "- On the contribution of neural networks and word embeddings in Natural Language Processing\n",
        "\n",
        "    https://medium.com/@josecamachocollados/on-the-contribution-of-neural-networks-and-word-embeddings-in-natural-language-processing-c8bb1b85c61c"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning our own word embeddings\n",
        "\n",
        "\n",
        "Customer reviews about a movie,\" including a mix of positive and negative sentiments.\n",
        "\n",
        "- \"Outstanding performances and a gripping storyline!\"\n",
        "- \"This movie exceeded my expectations, a must-watch!\"\n",
        "- \"Heartwarming and beautifully directed.\"\n",
        "- \"I can't stop thinking about this incredible film.\"\n",
        "- \"A cinematic masterpiece, truly remarkable.\"\n",
        "- \"Disappointing plot and lackluster acting.\"\n",
        "- \"Wasted my time, the worst movie I've seen.\"\n",
        "- \"Boring and unoriginal, a total letdown.\"\n",
        "- \"Terrible script, couldn't connect with the characters.\"\n",
        "- \"Avoid this film, it's a complete disaster.\"\n"
      ],
      "metadata": {
        "id": "7Jb9tXnLt3df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Initialize a list with movie review sentences\n",
        "movie_reviews = [\n",
        "    \"Loved it fantastic film\",\n",
        "    \"Great plot amazing acting\",\n",
        "    \"Disappointed boring storyline\",\n",
        "    \"Awesome movie worth watching\",\n",
        "    \"Weak performances not impressed\",\n",
        "    \"Incredible visuals, weak script\",\n",
        "    \"Enjoyed it a good time pass\",\n",
        "    \"Terrible film wasted my money\",\n",
        "    \"Superb acting clichéd plot\",\n",
        "    \"Entertaining but forgettable\"\n",
        "]\n",
        "\n",
        "# Split the sentences into tokens (words) using space as the delimiter\n",
        "tokenized_reviews = [review.lower().split() for review in movie_reviews]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(tokenized_reviews, vector_size=20, window=5, min_count=1)\n",
        "\n",
        "# Save the model for later use\n",
        "model.save(\"word2vec_myconnect.model\")\n",
        "\n",
        "# To load a saved model later, you can use:\n",
        "# model = Word2Vec.load(\"word2vec_myconnect.model\")\n",
        "\n",
        "# Find the vector representation of a word\n",
        "vector = model.wv['great']\n",
        "print(\"Vector for 'great':\", vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-XSW0VRt2qL",
        "outputId": "0f27896d-4947-4123-d20e-a197ababae51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'great': [-0.04121339  0.04649677 -0.0009883  -0.00983638  0.02301815 -0.02047658\n",
            "  0.01371557  0.03469983  0.03032713 -0.03755397  0.04691175  0.02335904\n",
            "  0.0198306  -0.03121753  0.0422999  -0.01075082  0.04412594 -0.02681001\n",
            " -0.0406471   0.03412279]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar words to a given word\n",
        "similar_words = model.wv.most_similar('storyline')\n",
        "print(\"Similar words to 'storyline':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzhN_R8ru5bt",
        "outputId": "fbe3a7ea-170e-47c7-bae6-2c328fbcc984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words to 'storyline': [('performances', 0.38172370195388794), ('time', 0.3259935677051544), ('watching', 0.31713858246803284), ('disappointed', 0.2947767674922943), ('entertaining', 0.24814458191394806), ('but', 0.2424035668373108), ('wasted', 0.2035253942012787), ('incredible', 0.19321590662002563), ('plot', 0.18296225368976593), ('money', 0.16422370076179504)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = model.wv.most_similar('terrible')\n",
        "print(\"Similar words to 'terrible':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWM2tF1MvIl_",
        "outputId": "510aa4d9-99db-433a-c544-912a939effa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words to 'terrible': [('disappointed', 0.4167259931564331), ('superb', 0.38196608424186707), ('worth', 0.3576344847679138), ('boring', 0.2781214118003845), ('performances', 0.2728114128112793), ('film', 0.257135808467865), ('awesome', 0.24820274114608765), ('my', 0.21764503419399261), ('plot', 0.16161365807056427), ('good', 0.08501722663640976)]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}