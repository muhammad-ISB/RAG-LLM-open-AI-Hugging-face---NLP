{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1bdeafd",
      "metadata": {
        "id": "b1bdeafd"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk import bigrams, FreqDist, ConditionalFreqDist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c065d0d2",
      "metadata": {
        "id": "c065d0d2",
        "outputId": "4de4b375-2a84-4fa4-fb2a-8d9595ace1fe",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mshos\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0756e51",
      "metadata": {
        "id": "a0756e51"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "\n",
        "## Keep your training documents in a folder named 'data'\n",
        "input_data_dir = \"data\"\n",
        "\n",
        "# String of punctuation without the full stop\n",
        "punctuation = string.punctuation.replace('.', '')  # Retain the full stop\n",
        "\n",
        "def is_hidden(filepath):\n",
        "    return os.path.basename(filepath).startswith('.')\n",
        "\n",
        "text_data=\"\"\n",
        "for filename in os.listdir(input_data_dir):\n",
        "    filepath = os.path.join(input_data_dir, filename)\n",
        "    if not is_hidden(filepath):\n",
        "        with open(filepath) as infile:\n",
        "            for line in infile:\n",
        "                if line.strip():  # Check if line is not just whitespace\n",
        "                    # Remove all punctuation except full stops\n",
        "                    for char in punctuation:\n",
        "                        line = line.replace(char, '')\n",
        "                    text_data += line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c2da5f",
      "metadata": {
        "id": "95c2da5f",
        "outputId": "d8fd7a48-da0b-4c31-f83f-1b7ff4c5f120"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8441834"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4929aabb",
      "metadata": {
        "id": "4929aabb"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text into words\n",
        "# Lowercasing for consistency\n",
        "words = nltk.word_tokenize(text_data.lower())\n",
        "\n",
        "# Generate bigrams\n",
        "bi_grams = list(bigrams(words))\n",
        "\n",
        "# Calculate frequency distribution for each bigram\n",
        "bi_gram_freq_dist = FreqDist(bi_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da52d781",
      "metadata": {
        "id": "da52d781",
        "outputId": "dc5fd732-0dac-41bf-ddc7-4081dbce3a16",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(('asian', 'exporters'), 1)\n",
            "(('exporters', 'fear'), 2)\n",
            "(('fear', 'damage'), 1)\n",
            "(('damage', 'from'), 8)\n",
            "(('from', 'u'), 27)\n"
          ]
        }
      ],
      "source": [
        "from itertools import islice\n",
        "# Print the first five elements of the dictionary\n",
        "first_five_items = list(islice(bi_gram_freq_dist.items(), 5))\n",
        "for item in first_five_items:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4290ff",
      "metadata": {
        "id": "1b4290ff"
      },
      "outputs": [],
      "source": [
        "# Compute conditional frequency distribution of bigrams\n",
        "bi_gram_freq = ConditionalFreqDist(bi_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af0da7b",
      "metadata": {
        "id": "2af0da7b",
        "outputId": "0d1da5b7-33b0-45fd-aead-76435438a781"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'gas': 216, 'rubber': 39, 'resources': 9, 'for': 3, 'float': 3, 'disasters': 2, 'that': 2, 'lt': 2, 'lower': 1, 'beverages': 1, ...})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bi_gram_freq['natural']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d611fb6",
      "metadata": {
        "id": "8d611fb6"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "topk=3\n",
        "# Create a dictionary to hold the top topk bigrams for each first word\n",
        "top_bigrams_per_first_word = {}\n",
        "\n",
        "# Iterate over the bigram frequency distribution\n",
        "for (first_word, second_word), freq in bi_gram_freq_dist.items():\n",
        "    # Initialize an empty heap for the first_word if it doesn't exist\n",
        "    if first_word not in top_bigrams_per_first_word:\n",
        "        top_bigrams_per_first_word[first_word] = []\n",
        "\n",
        "    # Add to the heap and maintain top topk\n",
        "    heapq.heappush(top_bigrams_per_first_word[first_word],\n",
        "                   (freq, second_word))\n",
        "    if len(top_bigrams_per_first_word[first_word]) > topk:\n",
        "        heapq.heappop(top_bigrams_per_first_word[first_word])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9847e087",
      "metadata": {
        "id": "9847e087",
        "outputId": "b7530b14-b102-42f6-ac5f-d030410c8ecc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(9, 'resources'), (216, 'gas'), (39, 'rubber')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_bigrams_per_first_word['natural']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7031ed37",
      "metadata": {
        "id": "7031ed37"
      },
      "outputs": [],
      "source": [
        "# Convert the heap to a simple list for each first word\n",
        "for first_word in top_bigrams_per_first_word:\n",
        "    sorted_bigrams = sorted(\n",
        "        top_bigrams_per_first_word[first_word], reverse=True)\n",
        "    top_bigrams_list = []\n",
        "    for freq, second_word in sorted_bigrams:\n",
        "        top_bigrams_list.append(second_word)\n",
        "    top_bigrams_per_first_word[first_word] = top_bigrams_list\n",
        "\n",
        "# Use these filtered bigrams to create a ConditionalFreqDist\n",
        "filtered_bi_grams = []\n",
        "for first_word in top_bigrams_per_first_word:\n",
        "    for second_word in top_bigrams_per_first_word[first_word]:\n",
        "        filtered_bi_grams.append((first_word, second_word))\n",
        "\n",
        "bi_gram_freq = ConditionalFreqDist(filtered_bi_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d187620",
      "metadata": {
        "id": "8d187620"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(word, num_words):\n",
        "    word =word.lower()\n",
        "    for _ in range(num_words):\n",
        "        print(word, end=' ')\n",
        "        next_words = [item for item, freq in bi_gram_freq[word].items()]\n",
        "        if len(next_words) > 0:\n",
        "            # Randomly choose a next word\n",
        "            word = random.choice(next_words)\n",
        "        else:\n",
        "            break  # Break if the word has no following words\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6d1728",
      "metadata": {
        "id": "cd6d1728",
        "outputId": "33a5e602-5e62-4ff8-b391-e6c0397aeb76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "asia pacific ltd said the dollar . the dollar and other major industrial countries and other than the u . 5 . the u card rate cut its board declared a new company . the dollar and a new york . the company . s lt xon unit of a new york investor asher s stock dividend to be the u card rate cut the dollar s lt bp s lt c and a new zealand ltd said . s lt xon . 5 mln dlrs in the u trans world bank said . the u card to the u \n"
          ]
        }
      ],
      "source": [
        "generate_sentence('Asia', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c0d450",
      "metadata": {
        "id": "11c0d450"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
