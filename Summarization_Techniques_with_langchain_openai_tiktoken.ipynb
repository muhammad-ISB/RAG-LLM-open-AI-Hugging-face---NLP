{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e38b551-44d2-4a31-ea5c-4eee119c48f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m1.7/1.9 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "J-KFB7J_u_3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d07f8e-0efa-4248-e008-b076ab027860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.315\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "wW6FD6FsT5Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "dNA4TsHpu6OM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb403026-bac3-45f6-d6db-e7b0eacc7f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Summarization Chain"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, PromptTemplate, LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D3RrGXkbAJh",
        "outputId": "dc02dfd0-1fc8-4ffc-e7bb-a367b636fa8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24\n",
            "drwxr-xr-x 1 root root 4096 Oct 16 12:18 .\n",
            "drwxr-xr-x 1 root root 4096 Oct 16 12:02 ..\n",
            "drwxr-xr-x 4 root root 4096 Oct 12 13:25 .config\n",
            "-rw-r--r-- 1 root root 6947 Oct 16 12:18 gpu_shortage\n",
            "drwxr-xr-x 1 root root 4096 Oct 12 13:25 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the doc\n",
        "with open('gpu_shortage') as f:\n",
        "    gpu_shortage_essay = f.read()"
      ],
      "metadata": {
        "id": "sCfCSX9sOv2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(gpu_shortage_essay)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GylknJF_SjOX",
        "outputId": "fa320ec2-468b-49fe-a672-bd106991c847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6751"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
        "texts = text_splitter.split_text(gpu_shortage_essay)"
      ],
      "metadata": {
        "id": "PcPzxvZZUTox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6KdLobvkLje",
        "outputId": "6a1f225a-81d1-4b9a-864e-630c6c162a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=t) for t in texts[:4]]"
      ],
      "metadata": {
        "id": "8L6ztJUJO2j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlSA_nIxdrSU",
        "outputId": "2abb5143-221a-4bb9-a443-84dfa973aac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='As compute-hungry generative AI shows no signs of slowing down, which companies are getting access to Nvidia’s hard-to-come-by, ultra-expensive, high-performance computing H100 GPU for large language model (LLM) training is becoming the “top gossip” of Silicon Valley, according to Andrej Karpathy, former director of AI at Tesla and now at OpenAI.\\n\\nKarpathy’s comments come at a moment where issues related to GPU access are even being discussed in big tech annual reports: In Microsoft’s annual report released last week, the company emphasized to investors that GPUs are a “critical raw material for its fast-growing cloud business” and added language about GPUs to a “risk factor for outages that can arise if it can’t get the infrastructure it needs.”'),\n",
              " Document(page_content='Karpathy took to the social network X (formerly Twitter) to re-share a widely circulated blog post thought to be authored by a poster on Hacker News that speculates “the capacity of large scale H100 clusters at small and large cloud providers is running out,” and that H100 demand will continue its trend till the end of 2024, at a minimum.\\n\\nThe author guesses that OpenAI might want 50,000 H100s, while Inflection wants 22,000, Meta “maybe 25k,” while “big clouds might want 30k each (Azure, Google Cloud, AWS, plus Oracle). Lambda and CoreWeave and the other private clouds might want 100k total. Anthropic, Helsing, Mistral and Character might want 10k each, he wrote.'),\n",
              " Document(page_content='The author said that these estimates are “total ballparks and guessing, and some of that is double-counting both the cloud and the end customer who will rent from the cloud. But that gets to about 432k H100s. At approx $35K a piece, that’s about $15B worth of GPUs. That also excludes Chinese companies like ByteDance (TikTok), Baidu and Tencent who will want a lot of H800s. There are also financial companies each doing deployments starting with hundreds of A100s or H100s and going to thousands of A/H100s: names like Jane Street, JP Morgan, Two Sigma, Citadel.”\\n\\nDemand for GPUs is like ‘Game of Thrones,’ says one VC\\nThe closest analogy to the battle to get access to AI chips is the television hit ‘Game of Thrones,’ David Katz, partner at Radical Ventures, told VentureBeat recently. “There’s this insatiable appetite for compute that’s required in order to run these models and large models,” he said.'),\n",
              " Document(page_content='Last year, Radical invested in CentML, which optimizes machine learning (ML) models to work faster and lower compute costs. CentML’s offering, he said, creates “a little bit more efficiency” in the market. In addition, it demonstrates that complex, billion-plus-parameter models can also run on legacy hardware.\\n\\n“So you don’t need the same volume of GPUs, or you don’t need the A100s necessarily,” he said. “From that perspective, it is essentially increasing the capacity or the supply of chips in the market.”\\n\\nHowever, those efforts may be more effective for those working on AI inference, rather than training LLMs from scratch, according to Sid Sheth, CEO of d-Matrix, which is building a platform to save money on inference by doing more processing in the computer’s memory, rather than on a GPU.')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  3 types of CombineDocuments Chains\n",
        "\n",
        "[Taken from the LangChain Docs](https://langchain.readthedocs.io/en/latest/modules/indexes/combine_docs.html)"
      ],
      "metadata": {
        "id": "IczFCIKaLCet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize Simple with map_reduce"
      ],
      "metadata": {
        "id": "a6kbcja8O7-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
        "\n",
        "{text}\n",
        "\n",
        "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
        "\n",
        "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "4jVWsy4cXibp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Stuffing\n",
        "Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n",
        "\n",
        "**Pros:** Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n",
        "\n",
        "**Cons:** Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n",
        "\n",
        "The main downside of this method is that **it only works one smaller pieces of data.**  Once you are working with many pieces of data, this approach is no longer feasible. The next two approaches are designed to help deal with that.\n",
        "\n"
      ],
      "metadata": {
        "id": "9o7FgrlMhuW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=BULLET_POINT_PROMPT)\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "\n",
        "wrapped_text = textwrap.fill(output_summary,\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrtSb8VphVG-",
        "outputId": "65af5045-db1f-4d4c-9726-01c15932a57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- Nvidia's H100 GPU is in high demand for large language model (LLM) training\n",
            "- Companies such as\n",
            "OpenAI, Inflection, Meta, Azure, Google Cloud, AWS, Oracle, Lambda, CoreWeave, Anthropic, Helsing,\n",
            "Mistral, Character, ByteDance, Baidu, Tencent, Jane Street, JP Morgan, Two Sigma, and Citadel are\n",
            "all vying for access to the H100 GPU\n",
            "- Demand for GPUs is likened to the television show 'Game of\n",
            "Thrones'\n",
            "- Radical Ventures has invested in CentML to optimize ML models and increase the supply of\n",
            "chips in the market\n",
            "- d-Matrix is building a platform to save money on inference by doing more\n",
            "processing in the computer's memory, rather than on a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### map_reduce summarization with custom prompt"
      ],
      "metadata": {
        "id": "cJ_ig4J_PYbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with a custom prompt\n",
        "prompt_template = \"\"\"Write a concise summary of the following:\n",
        "\n",
        "{text}\n",
        "\n",
        "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])\n",
        "\n",
        "## with intermediate steps\n",
        "chain = load_summarize_chain(OpenAI(temperature=0),\n",
        "                             chain_type=\"map_reduce\",\n",
        "                             return_intermediate_steps=True,\n",
        "                             map_prompt=PROMPT,\n",
        "                             combine_prompt=PROMPT)\n",
        "\n",
        "output_summary = chain({\"input_documents\": docs}, return_only_outputs=True)\n",
        "wrapped_text = textwrap.fill(output_summary['output_text'],\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUFsaodtPkl4",
        "outputId": "79aa6b32-5eb1-45dc-e172-cfe678e73d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "• Nvidia's H100 GPU is becoming a hot topic in Silicon Valley due to its high-performance computing\n",
            "capabilities.\n",
            "• Andrej Karpathy, former director of AI at Tesla, has commented on the issue.\n",
            "•\n",
            "Microsoft's annual report has highlighted GPUs as a \"critical raw material\" for its cloud business.\n",
            "• Estimates suggest that 432,000 H100s will be purchased, worth approximately $15 billion.\n",
            "• Radical\n",
            "Ventures has invested in CentML to optimize ML models and create more efficiency in the market.\n",
            "•\n",
            "d-Matrix is building a platform to save money on inference by doing more processing in the\n",
            "computer's memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_text = textwrap.fill(output_summary['intermediate_steps'][2],\n",
        "                             width=100,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvksjoTytSCb",
        "outputId": "1d0f0fd6-3043-430a-d7c8-5d5ea7773248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- David Katz, partner at Radical Ventures, compared the battle to get access to AI chips to the\n",
            "television hit ‘Game of Thrones’\n",
            "- He noted the “insatiable appetite for compute” needed to run\n",
            "large models\n",
            "- Estimates suggest that 432,000 H100s will be purchased, worth approximately $15\n",
            "billion\n",
            "- Chinese companies such as ByteDance, Baidu and Tencent are expected to purchase H800s\n",
            "-\n",
            "Financial companies such as Jane Street, JP Morgan, Two Sigma and Citadel are expected to purchase\n",
            "hundreds to thousands of A/H100s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With the 'refine' CombineDocument Chain"
      ],
      "metadata": {
        "id": "caaAmomfPv9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refine\n",
        "This method involves **an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document**, asking the LLM to refine the output based on the new document.\n",
        "\n",
        "**Pros:** Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n",
        "\n",
        "**Cons:** Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents."
      ],
      "metadata": {
        "id": "39P6zjy9lT5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary, width=100)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ji2D8Q0P2gh",
        "outputId": "fb843330-f805-4c69-e5d3-54b648adfe54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Nvidia's H100 GPU, a hard-to-come-by and expensive high-performance computing device, is becoming\n",
            "a hot topic in Silicon Valley. Andrej Karpathy, former director of AI at Tesla and now at OpenAI,\n",
            "has commented on the issue, and Microsoft's annual report has also addressed the importance of GPUs\n",
            "for its cloud business. Karpathy took to the social network X (formerly Twitter) to re-share a\n",
            "widely circulated blog post thought to be authored by a poster on Hacker News that speculates “the\n",
            "capacity of large scale H100 clusters at small and large cloud providers is running out,” and that\n",
            "H100 demand will continue its trend till the end of 2024, at a minimum. The post estimates that\n",
            "OpenAI might want 50,000 H100s, while Inflection wants 22,000, Meta “maybe 25k,” and “big clouds”\n",
            "such as Azure, Google Cloud, AWS, and Oracle might want 30k each. Additionally, Lambda, CoreWeave,\n",
            "Anthropic, Helsing, Mistral, and Character might want 10k each. Last year, Radical invested in\n",
            "CentML, which optimizes machine learning (ML) models to work faster and lower\n"
          ]
        }
      ]
    }
  ]
}